{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers Past Scraper\n",
    "\n",
    "This notebook is designed to scrape a specific [periodical](https://paperspast.natlib.govt.nz/periodicals) from [Papers Past](https://paperspast.natlib.govt.nz/) and to create a corpus based on the OCRd text. Obviously DigitalNZ is a better way to capture data from PapersPast, but not everything is available by the DigitalNZ API.\n",
    "\n",
    "In the case of this notebook I am scraping the [New Zealand Police Gazette](https://paperspast.natlib.govt.nz/periodicals/new-zealand-police-gazette). Note that each periodical has background information and details of copyright and/or creative commons licenses (e.g. see the [New Zealand Police Gazette](https://paperspast.natlib.govt.nz/periodicals/new-zealand-police-gazette) page as an example). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main requirement that requires configuration is [Selenium](https://selenium.dev/). Selenium is used to automate a web browser. Requests using the `requests` library were blocked.  \n",
    "\n",
    "Documentation that is relevant for installing Selenium and a driver is available at the [Selenium PyPI page](https://pypi.org/project/selenium/).  \n",
    "\n",
    "In this case I am using the [Geckodriver](https://github.com/mozilla/geckodriver) to automate Firefox. Note: [Geckodriver binaries available here](https://github.com/mozilla/geckodriver/releases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure the driver path ...\n",
    "driver_path = 'C:/applications/geckodriver' # these need to be installed - see note above about Geckodriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory paths\n",
    "issues_dir = r'cache/issues'\n",
    "contents_dir = r'cache/contents'\n",
    "corpus_dir = r'corpus'\n",
    "\n",
    "# make sure the required directories are created\n",
    "if not os.path.exists(issues_dir):\n",
    "    os.makedirs(issues_dir)\n",
    "if not os.path.exists(contents_dir):\n",
    "    os.makedirs(contents_dir)\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the start url\n",
    "\n",
    "This should be the first issue of the publication in question. The rest of the issues are accessed automatically by crawling all issues using the 'Next issue' link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://paperspast.natlib.govt.nz/periodicals/new-zealand-police-gazette/1877/07/02'\n",
    "\n",
    "# this sets a base_url for use in resolving urls extracted from the page ...\n",
    "url = urllib.parse.urlparse(start_url)\n",
    "base_url = url.scheme + '://' + url.netloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and save all issue pages\n",
    "Starting at the `start_url` this code clicks through all the separate issues of the publication and stores each issue page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path = driver_path)\n",
    "next_url = start_url\n",
    "while next_url != False:\n",
    "    print('Retrieve', next_url)\n",
    "    driver.get(next_url)\n",
    "    \n",
    "    # build filename for cache\n",
    "    url = urllib.parse.urlparse(next_url)\n",
    "    filename = url.path.strip('/').replace('/' ,'-') + '.txt'\n",
    "\n",
    "    # cache file\n",
    "    f = open(issues_dir + '/' + filename, 'w', encoding='utf-8')\n",
    "    f.write(driver.page_source)\n",
    "    f.close()\n",
    "    \n",
    "    # extract next link using beautiful soup\n",
    "    soup = bs.BeautifulSoup(driver.page_source)\n",
    "    link = soup.select_one(\"div.show-for-medium div.pager__right a\")\n",
    "    if link is None:\n",
    "        next_url = False\n",
    "    else:\n",
    "        next_url = urllib.parse.urljoin(base_url, link['href']) \n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and save all contents pages\n",
    "This code extracts the content pages from the issue pages, requests the content pages and saves them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path = driver_path)\n",
    "for filename in glob.glob(os.path.join(issues_dir, '*.txt')):\n",
    "    # open file\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # extract links\n",
    "    soup = bs.BeautifulSoup(contents)\n",
    "    for link in soup.select(\"ul.issue__contents li a\"):\n",
    "        next_url = urllib.parse.urljoin(base_url, link['href'])\n",
    "        print('Retrieve', next_url)\n",
    "        driver.get(next_url)\n",
    "\n",
    "        # build filename for cache\n",
    "        url = urllib.parse.urlparse(next_url)\n",
    "        filename = url.path.strip('/').replace('/' ,'-') + '.txt'\n",
    "\n",
    "        # cache file\n",
    "        f = open(contents_dir + '/' + filename, 'w', encoding='utf-8')\n",
    "        f.write(driver.page_source)\n",
    "        f.close()\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write corpus\n",
    "This reads through the scraped content files, extracts the content, strips tags and writes a file. If the content for the page is blank (e.g. for the masthead image), then there will be no file written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counter = 0\n",
    "for filename in glob.glob(os.path.join(contents_dir, '*.txt')):\n",
    "    # open file\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # extract content\n",
    "    text = ''\n",
    "    soup = bs.BeautifulSoup(contents)\n",
    "    content = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "    text = content.get_text(\"\\n\")\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if text != '':\n",
    "        # write corpus file\n",
    "        file_counter += 1\n",
    "        f = open(corpus_dir + '/' +  os.path.basename(filename), 'w', encoding='utf-8')\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        \n",
    "print(file_counter,'corpus files written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
