{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers Past Scraper\n",
    "\n",
    "This notebook is designed to scrape a specific publication from [Papers Past](https://paperspast.natlib.govt.nz/). In this case: [New Zealand Police Gazette](https://paperspast.natlib.govt.nz/periodicals/new-zealand-police-gazette)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure the driver path ...\n",
    "driver_path = 'C:/applications/geckodriver' # these need to be installed obviously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory paths\n",
    "issues_dir = r'cache/issues'\n",
    "contents_dir = r'cache/contents'\n",
    "corpus_dir = r'corpus'\n",
    "\n",
    "# make sure the required directories are created\n",
    "if not os.path.exists(issues_dir):\n",
    "    os.makedirs(issues_dir)\n",
    "if not os.path.exists(contents_dir):\n",
    "    os.makedirs(contents_dir)\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the start url\n",
    "\n",
    "This should be the first issue of the publication in question. The rest of the issues are accessed automatically by crawling all issues using the 'Next issue' link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://paperspast.natlib.govt.nz/periodicals/new-zealand-police-gazette/1877/07/02'\n",
    "\n",
    "# this sets a base_url for use in resolving urls extracted from the page ...\n",
    "url = urllib.parse.urlparse(start_url)\n",
    "base_url = url.scheme + '://' + url.netloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and save all issue pages\n",
    "Starting at the `start_url` this code clicks through all the separate issues of the publication and stores each issue page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path = driver_path)\n",
    "next_url = start_url\n",
    "while next_url != False:\n",
    "    print('Retrieve', next_url)\n",
    "    driver.get(next_url)\n",
    "    \n",
    "    # build filename for cache\n",
    "    url = urllib.parse.urlparse(next_url)\n",
    "    filename = url.path.strip('/').replace('/' ,'-') + '.txt'\n",
    "\n",
    "    # cache file\n",
    "    f = open(issues_dir + '/' + filename, 'w', encoding='utf-8')\n",
    "    f.write(driver.page_source)\n",
    "    f.close()\n",
    "    \n",
    "    # extract next link using beautiful soup\n",
    "    soup = bs.BeautifulSoup(driver.page_source)\n",
    "    link = soup.select_one(\"div.show-for-medium div.pager__right a\")\n",
    "    if link is None:\n",
    "        next_url = False\n",
    "    else:\n",
    "        next_url = urllib.parse.urljoin(base_url, link['href']) \n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and save all contents pages\n",
    "This code extracts the content pages from the issue pages, requests the content pages and saves them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path = driver_path)\n",
    "for filename in glob.glob(os.path.join(issues_dir, '*.txt')):\n",
    "    # open file\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # extract links\n",
    "    soup = bs.BeautifulSoup(contents)\n",
    "    for link in soup.select(\"ul.issue__contents li a\"):\n",
    "        next_url = urllib.parse.urljoin(base_url, link['href'])\n",
    "        print('Retrieve', next_url)\n",
    "        driver.get(next_url)\n",
    "\n",
    "        # build filename for cache\n",
    "        url = urllib.parse.urlparse(next_url)\n",
    "        filename = url.path.strip('/').replace('/' ,'-') + '.txt'\n",
    "\n",
    "        # cache file\n",
    "        f = open(contents_dir + '/' + filename, 'w', encoding='utf-8')\n",
    "        f.write(driver.page_source)\n",
    "        f.close()\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write corpus\n",
    "This reads through the scraped content files, extracts the content, strips tags and writes a file. If the content for the page is blank (e.g. for the masthead image), then there will be no file written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counter = 0\n",
    "for filename in glob.glob(os.path.join(contents_dir, '*.txt')):\n",
    "    # open file\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # extract content\n",
    "    text = ''\n",
    "    soup = bs.BeautifulSoup(contents)\n",
    "    content = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "    text = content.get_text(\"\\n\")\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if text != '':\n",
    "        # write corpus file\n",
    "        file_counter += 1\n",
    "        f = open(corpus_dir + '/' +  os.path.basename(filename), 'w', encoding='utf-8')\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        \n",
    "print(file_counter,'corpus files written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
